{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ddc7a60",
   "metadata": {},
   "source": [
    "### 1. Extract subject from document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a013c",
   "metadata": {},
   "source": [
    "#### The first approach to identify the subject of a document is to find the intersection between nouns and named entites.\n",
    "#### 1. First I will find all nouns from text using pos tagger from nltk library and select top 10 of them.\n",
    "#### 2. Then, I will find all named entities using ner from nltk library and again select top 10 entites.\n",
    "#### 3. Finally, I will find their intersection and return the enties which are in the list of top nouns\n",
    "#### This would give me potential subject of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "875a0ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Qquentin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Qquentin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Qquentin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Qquentin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Qquentin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1ca163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenize_data(text):\n",
    "    text = re.sub('[^A-Za-z -]+', ' ', text)\n",
    "    text = ' '.join(text.split())\n",
    "    text = ' '.join([i for i in text.split() if i not in stop])\n",
    "    sents = [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text)]\n",
    "    \n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd283792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_nouns(text):\n",
    "    # get common words\n",
    "    words = []\n",
    "    for sent in text:\n",
    "        for word in sent:\n",
    "            words.append(word)\n",
    "    word_counter = Counter(words)\n",
    "    common_words = word_counter.most_common(10)\n",
    "    \n",
    "    # get common nouns\n",
    "    common_nouns = []\n",
    "    for w, c in common_words:\n",
    "        if nltk.pos_tag([w])[0][1] in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "            common_nouns.append(w.lower())\n",
    "            \n",
    "    return common_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "746eb776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_named_entities(sents):\n",
    "    # extract all named entities\n",
    "    named_entities = []\n",
    "    pos_sents = [nltk.pos_tag(sent) for sent in sents]\n",
    "    for pos_sent in pos_sents:\n",
    "        for chunk in nltk.ne_chunk(pos_sent):\n",
    "            if type(chunk) == nltk.tree.Tree:\n",
    "                named_entities.append(' '.join([c[0] for c in chunk]).lower())\n",
    "                \n",
    "    # get only the most common entities\n",
    "    entity_counter = Counter(named_entities)\n",
    "    common_entities = entity_counter.most_common(10)\n",
    "    common_word_entities = [word for word, counts in common_entities]\n",
    "    \n",
    "    return common_word_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2724d1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_subject(document):\n",
    "    sents = clean_tokenize_data(document)\n",
    "    common_nouns = get_common_nouns(sents)\n",
    "    common_named_entities = get_common_named_entities(sents)\n",
    "    print('common nouns from text: ', common_nouns)\n",
    "    print('common named entities from text: ', common_named_entities)\n",
    "    subject = []\n",
    "    for named_entity in common_named_entities:\n",
    "        if named_entity in common_nouns:\n",
    "            subject.append(named_entity)\n",
    "    return subject "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b23076ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('document.txt', 'r', encoding=\"utf8\") as f:\n",
    "    document = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e128ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common nouns from text:  ['jots', 'time', 'tool', 'task', 'projects', 'content', 'jot', 'everything']\n",
      "common named entities from text:  ['jots', 'jot jot', 'jeremy apple park jots', 'sdk']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['jots']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_document_subject(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "107aec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from min_dalle import MinDalle\n",
    "\n",
    "Dalle_model = MinDalle(is_mega=False, is_verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ae9a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_stream = Dalle_model.generate_image_stream(\n",
    "    text='jots time tool task projects content jot everything',\n",
    "    seed=-1,\n",
    "    grid_size=3,\n",
    "    progressive_outputs=True,\n",
    "    is_seamless=False,\n",
    "    temperature=1,\n",
    "    top_k=256,\n",
    "    supercondition_factor=16,\n",
    "    is_verbose=False\n",
    ")\n",
    "\n",
    "for image in image_stream:\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d025a4",
   "metadata": {},
   "source": [
    "#### The second approach is to use language models to extract not just words but key phrases from text (e.g. https://arxiv.org/abs/2112.08547)\n",
    "#### I can load pretrained model which extracts key words, but of course it is possible to implement a custom model myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "955344a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TokenClassificationPipeline, AutoModelForTokenClassification, AutoTokenizer\n",
    "from transformers.pipelines import AggregationStrategy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "24709758",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyphraseExtractionPipeline(TokenClassificationPipeline):\n",
    "    def __init__(self, model, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            model=AutoModelForTokenClassification.from_pretrained(model),\n",
    "            tokenizer=AutoTokenizer.from_pretrained(model),\n",
    "            *args,\n",
    "            **kwargs)\n",
    "\n",
    "    def postprocess(self, model_outputs):\n",
    "        results = super().postprocess(\n",
    "            model_outputs=model_outputs,\n",
    "            aggregation_strategy=AggregationStrategy.SIMPLE,\n",
    "        )\n",
    "        return np.unique([result.get(\"word\").strip() for result in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "121a3dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f41111e5194098bdac4498470e7db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b8dc87e851846d7a342a5706dac87af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.32G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020bcd0f3a524a5097e5d6a3ac324057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/1.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665f3028cdf7455db472ca87fcf1a79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/780k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517f198216184532860ef12bbe625869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb9fa4e718c49ce9c7dc287914cb2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.01M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649b97b9f8494f58aa74f2b4818c9c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"ml6team/keyphrase-extraction-kbir-inspec\"\n",
    "extractor = KeyphraseExtractionPipeline(model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c4618e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C', 'Interfax news agency', 'Kamchatka peninsula',\n",
       "       'Klyuchevskaya Sopka volcano', 'Kyrgyzstan', 'Russia',\n",
       "       'Temperatures', 'Tian Shan mountains',\n",
       "       'Unesco world heritage site', 'avalanche',\n",
       "       'civil defence authority', 'freezing winds', 'gale force winds',\n",
       "       'kaya Sopka', 'satellite phone'], dtype='<U27')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e831a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91cb01ea",
   "metadata": {},
   "source": [
    "### 2. Find similar text from document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458d270f",
   "metadata": {},
   "source": [
    "#### For this problem we need embeddings. We can either use word vectors and average them out or use sentence embeddings\n",
    "#### I decided to use sentence embeddings from HuggingFace which uses SentenceBERT (https://arxiv.org/abs/1908.10084)\n",
    "#### Then, I will find the closest sentence in the document for the given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bb27c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f23d373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenize_data(text):\n",
    "    text = re.sub('[^A-Za-z -.]+', ' ', text)\n",
    "    text = ' '.join(text.split())\n",
    "    text = ' '.join([i for i in text.split() if i not in stop])\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    sents = [' '.join(nltk.word_tokenize(sent)) for sent in sents]\n",
    "    \n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cf0267d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_scores(model, sents):\n",
    "    query = sents[1]\n",
    "    score_list = []\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    for idx in range(len(sents)):\n",
    "        embedding = model.encode(sents[idx], convert_to_tensor=True)  \n",
    "        score = util.pytorch_cos_sim(query_embedding, embedding).item()\n",
    "        #print(sents[idx], util.pytorch_cos_sim(query_embedding, embeddings_list[idx]).item())\n",
    "        score_list.append((idx, score))\n",
    "        \n",
    "    sorted_scores = sorted(score_list, key=lambda x:(-x[1], x[0]))\n",
    "    return sorted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "446f07f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('document.txt', 'r', encoding=\"utf8\") as f:\n",
    "    document = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d916b20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top closest sentences to the query \"Six people initially reported killed , according officials Kamchatka peninsula , six believed stranded party , included two guides .\" are the followings:\n",
      ">>>>>>\n",
      "\"Eight climbers died attempting scale Klyuchevskaya Sopka volcano Russia far east , according local officials , freezing winds halted rescue attempt .\" With the score:  0.4609769582748413\n",
      "\"But Interfax news agency said two died , quoting Roman Vasilevsky , Kamchatka territory deputy prime minister .\" With the score:  0.4139235019683838\n",
      "\"The party set Tuesday climb mountain Eurasia highest active volcano ran trouble Saturday group fell death almost , m , authorities said .\" With the score:  0.3748098313808441\n",
      "\"One person thought broken leg , added .\" With the score:  0.3098146915435791\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "top = 5\n",
    "\n",
    "sents = clean_tokenize_data(document)\n",
    "sorted_scores = get_similarity_scores(model, sents)\n",
    "\n",
    "print('Top closest sentences to the query \"%s\" are the followings:' %sents[1])\n",
    "print('>>>>>>')\n",
    "for idx, score in sorted_scores[1:top]:\n",
    "    print('\"%s\" With the score: ' %sents[idx], score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17774f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "f800a277f73fad4896a460eaa489683c53f96f0fdb097dbced45fa55364e6c2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
